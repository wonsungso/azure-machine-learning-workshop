# ğŸ§ª Lab 03 â€” Triton Inference Serverë¡œ ëª¨ë¸ ë°°í¬

---

## ğŸ¯ Lab ëª©í‘œ

ì´ Labì—ì„œëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ Azure Machine Learningì˜
Managed Online Endpointë¡œ ë°°í¬í•˜ê³  Triton Inference Serverë¥¼ í†µí•´ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì™„ë£Œ í›„ ìƒíƒœ:

- Managed Online Endpoint ìƒì„±
- Triton Deployment êµ¬ì„±
- ì‹¤ì‹œê°„ Inference í…ŒìŠ¤íŠ¸

ì´ ë‹¨ê³„ëŠ” Workshopì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ë¡œ, í•™ìŠµëœ ëª¨ë¸ì„ ì‹¤ì œ ì„œë¹„ìŠ¤ í˜•íƒœë¡œ ë°°í¬í•©ë‹ˆë‹¤.

---

## ğŸ§­ Workshop ì „ì²´ íë¦„ì—ì„œì˜ ìœ„ì¹˜

```
Setup
   â†“
RAPIDS ë°ì´í„° ì „ì²˜ë¦¬
   â†“
PyTorch ëª¨ë¸ í•™ìŠµ
   â†“
[í˜„ì¬] Triton Endpoint ë°°í¬
```

---

## ğŸ§  Triton Inference Serverë€?

NVIDIA Tritonì€ ê³ ì„±ëŠ¥ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ ì„œë²„ì…ë‹ˆë‹¤.

íŠ¹ì§•:

- GPU ìµœì í™” Inference
- ONNX / PyTorch / TensorRT ì§€ì›
- ì‹¤ì‹œê°„ API Endpoint ì œê³µ

ê°„ë‹¨íˆ ë§í•˜ë©´:

```
Training = ëª¨ë¸ ìƒì„±
Triton   = ëª¨ë¸ ì„œë¹„ìŠ¤í™”
```

---

# 1ï¸âƒ£ Managed Online Endpoint ìƒì„±

## Step 1. Endpoints ë©”ë‰´ ì´ë™

Azure ML Studio ì¢Œì¸¡:

```
Assets â†’ Endpoints
```

---

## Step 2. Online Endpoint ìƒì„±

```
+ Create â†’ Real-time endpoint
```

ì„¤ì •:

```
Endpoint name : ep-dl-workshop
Authentication: Key
```

Create í´ë¦­

---

## â³ ìƒì„± ì‹œê°„

ì•½ 2~3ë¶„

---

## âœ… Checkpoint

Endpoint ëª©ë¡ì— ì•„ë˜ê°€ ë³´ì´ë©´ ì •ìƒì…ë‹ˆë‹¤.

```
ep-dl-workshop
```

---

# 2ï¸âƒ£ Triton Deployment ìƒì„±

## Step 1. Deployment ì¶”ê°€

Endpoint ìƒì„¸ í™”ë©´:

```
+ Add deployment
```

ì„¤ì •:

```
Deployment name : triton-deploy
Instance type   : Standard_DS3_v2
Instance count  : 1
Model           : í•™ìŠµëœ ëª¨ë¸ ì„ íƒ
```

Inference Server:

```
Triton
```

Create í´ë¦­

---

## ğŸ’¡ Workshop Tip

ì—¬ê¸°ì„œ ì°¸ê°€ìì—ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”:

```
Endpoint = API ì£¼ì†Œ
Deployment = ì‹¤ì œ ì‹¤í–‰ë˜ëŠ” VM
```

---

## â³ Deployment ì¤€ë¹„ ê³¼ì •

ë‚´ë¶€ì ìœ¼ë¡œ:

```
Container ìƒì„±
Model ë¡œë“œ
Triton ì„œë²„ ì‹œì‘
```

ì´ ìë™ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.

---

## âœ… Checkpoint

Deployment ìƒíƒœê°€ ì•„ë˜ì²˜ëŸ¼ ë³€ê²½ë©ë‹ˆë‹¤.

```
Creating â†’ Healthy
```

Healthy ìƒíƒœê°€ ë˜ë©´ ì¤€ë¹„ ì™„ë£Œì…ë‹ˆë‹¤.

---

# 3ï¸âƒ£ Endpoint í…ŒìŠ¤íŠ¸

## Step 1. Test íƒ­ ì´ë™

Endpoint í™”ë©´ ìƒë‹¨:

```
Test
```

---

## Step 2. Sample Payload ì…ë ¥

```json
{"input":[1,2,3]}
```

Run í´ë¦­

---

## âœ… Checkpoint

Response JSONì´ ë°˜í™˜ë˜ë©´ ì„±ê³µì…ë‹ˆë‹¤.

---

# ğŸ§± ìµœì¢… ì•„í‚¤í…ì²˜ êµ¬ì„±

```
Azure ML Workspace
        â”œâ”€â”€ Compute Instance
        â”œâ”€â”€ GPU Compute Cluster
        â””â”€â”€ Managed Online Endpoint
                â””â”€â”€ Triton Deployment
```

---

# âš ï¸ Troubleshooting

## âŒ Deployment ìƒì„± ì‹¤íŒ¨

ê°€ëŠ¥ ì›ì¸:

- Instance quota ë¶€ì¡±
- ëª¨ë¸ ì„ íƒ ì˜¤ë¥˜

í•´ê²°:

VM Sizeë¥¼ Standard_DS2_v2ë¡œ ë‚®ì¶° ì¬ì‹œë„

---

## âŒ Endpoint ì‘ë‹µ ì—†ìŒ

Deployment ìƒíƒœê°€ Healthyì¸ì§€ í™•ì¸ í›„ ì¬ì‹œë„

---

# ğŸ¤ Workshop ì§„í–‰ í¬ì¸íŠ¸

ì´ Labì—ì„œ ê°•ì¡°í•  ë‚´ìš©:

- Trainingê³¼ DeploymentëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ë‹¨ê³„
- Azure MLì€ ëª¨ë¸ì„ ë°”ë¡œ APIë¡œ ë°°í¬ ê°€ëŠ¥
- Tritonì€ GPU Inference ìµœì í™” ì„œë²„

---

# ğŸ§¹ Workshop ì¢…ë£Œ í›„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ì¤‘ìš”)

Azure Portalì—ì„œ Resource Group ì‚­ì œ:

```
rg-aml-dl-workshop
```

GPU ë° Endpoint ë¹„ìš©ì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

# ğŸ‰ Workshop ì™„ë£Œ

ì¶•í•˜í•©ë‹ˆë‹¤!

ì´ë²ˆ Workshopì—ì„œ ë‹¤ìŒì„ ê²½í—˜í–ˆìŠµë‹ˆë‹¤:

- Azure ML Workspace êµ¬ì„±
- RAPIDS GPU ë°ì´í„° ì „ì²˜ë¦¬
- PyTorch GPU í•™ìŠµ
- Triton Endpoint ë°°í¬

ì‘ì„±ì¼: 2026-02-19
